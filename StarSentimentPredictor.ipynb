{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StarSentimentPredictor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw-2G-12ipsA"
      },
      "source": [
        "class Star_Sentiment_Model():\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ########################################################################\n",
        "    #Convert to the required format of the GPT2 model\n",
        "    class DataConverter():\n",
        "      def __init__(self,dataset, train, use_tokenizer):\n",
        "\n",
        "        \n",
        "        self.texts = []\n",
        "        self.labels = []\n",
        "        \n",
        "        if train ==True:\n",
        "          self.texts = dataset.trX\n",
        "          print(len(self.texts))\n",
        "          self.labels= dataset.trY\n",
        "        elif train== False:\n",
        "          self.texts = dataset.vaX\n",
        "          self.labels= dataset.vaY\n",
        "        else:\n",
        "          self.texts = dataset.teX\n",
        "          self.labels= dataset.teY  \n",
        "        \n",
        "        self.n_examples = self.labels \n",
        "        return\n",
        "        \n",
        "      def __len__(self):\n",
        "        \n",
        "        return (len(self.n_examples))\n",
        "\n",
        "      def __getitem__(self, item):\n",
        "\n",
        "\n",
        "        #Returns: Dictionary of inputs that contain text and asociated labels.\n",
        "        return {'text':self.texts[item],\n",
        "                'label':self.labels[item]}\n",
        "\n",
        "\n",
        "    #colate function for dataloader\n",
        "    class Gpt2Collator:\n",
        "          \n",
        "          def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
        "\n",
        "            # Tokenizer to be used inside the class.\n",
        "            self.use_tokenizer = use_tokenizer\n",
        "            # Check max sequence length.\n",
        "            self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
        "            # Label encoder used inside the class.\n",
        "            self.labels_encoder = labels_encoder\n",
        "\n",
        "            return\n",
        "            \n",
        "          def __call__(self, sequences):\n",
        "            \n",
        "            import torch\n",
        "            # Get all texts from sequences list.\n",
        "            texts = [sequence['text'] for sequence in sequences]\n",
        "            # Get all labels from sequences list.\n",
        "            labels = [sequence['label'] for sequence in sequences]\n",
        "            # Encode all labels using label encoder.\n",
        "            #labels = [self.labels_encoder[label] for label in labels]\n",
        "            # Call tokenizer on all texts to convert into tensors of numbers with \n",
        "            # appropriate padding.\n",
        "            inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
        "            # Update the inputs with the associated encoded labels as tensor.\n",
        "            inputs.update({'labels':torch.tensor(labels)})\n",
        "            \n",
        "            return inputs\n",
        "\n",
        "    #class to load my new dataset\n",
        "    class Inputdatareader:\n",
        "  \n",
        "      def __init__(self, topic=None):\n",
        "        seed = 3535999445\n",
        "        #Provide the path to the Input data file\n",
        "        \n",
        "        self.teX,self.teY = self.stance(  topic=topic)\n",
        "    \n",
        "      # Cleaning the tweets and converting labels to numbers.\n",
        "      def stance(self, topic=None):\n",
        "        import torch\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "\n",
        "        #path='/content/drive/MyDrive/BT_data/patagonia dataset - Copy.csv'\n",
        "        def clean_ascii(text):\n",
        "            # function to remove non-ASCII chars from data\n",
        "            return ''.join(i for i in text if ord(i) < 128)\n",
        "        #orig = pd.read_csv(path, encoding = \"utf-8\")\n",
        "        #Generate random values for True Labels. Because the model need something as input. But we just ignore it later.\n",
        "        orig = pd.DataFrame(columns=['text', 'Labels'])\n",
        "        orig['text']=[Input_tweet]\n",
        "        orig['labels']=np.random.randint(0,2, size=len(orig['text']))\n",
        "        orig['text'] = orig[\"text\"].apply(clean_ascii)\n",
        "        X = orig['text']\n",
        "        Y= orig['labels']   \n",
        "        return X,Y\n",
        "\n",
        "    # push the patagonia dataset through the model\n",
        "    def test(self,dataloader,saved_model, device_):\n",
        "      \n",
        "      from tqdm.notebook import tqdm\n",
        "      import torch\n",
        "      import math\n",
        "      # Use global variable for model.\n",
        "      global model\n",
        "      #the_model = torch.load('/content/drive/MyDrive/BT_data/model2.pt', map_location=torch.device('cpu'))\n",
        "      # Load saved model ('the_model') instead of new model\n",
        "      model= saved_model\n",
        "\n",
        "      predictions_labels = []\n",
        "      true_labels = []\n",
        "      total_loss = 0\n",
        "      all_probs=[]\n",
        "      sum=0.0\n",
        "\n",
        "      # Put the model in evaluation mode\n",
        "      model.eval()\n",
        "      \n",
        "      # Evaluate data for one epoch\n",
        "      for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "\n",
        "        # add original labels\n",
        "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
        "        with torch.no_grad():   \n",
        "            # Send batch of new data to the model      \n",
        "            outputs = model(**batch)\n",
        "            # Model outputs obtained are loss and Logit\n",
        "            loss, logits = outputs[:2]\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            total_loss += loss.item()\n",
        "            predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
        "            \n",
        "            # Converting logits(logistic regression) to Probabilities(similar to softmax)\n",
        "            for i in logits:\n",
        "              #logits1=max(i)\n",
        "              sum=0.0\n",
        "              for j in i:\n",
        "                sum+= float((math.exp(j)))\n",
        "              probs=[]\n",
        "              prob=0.0\n",
        "              for j in i:\n",
        "                prob=math.exp(j)/sum\n",
        "                probs.append(prob)\n",
        "            \n",
        "\n",
        "            # update list\n",
        "            predictions_labels += predict_content\n",
        "          \n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_epoch_loss = total_loss / len(dataloader)\n",
        "      \n",
        "      #De-Tokenizer and function to determine overall public opinion towards this topic\n",
        "      labels=predictions_labels\n",
        "      if predictions_labels==[0]:\n",
        "        predictions_labels='NEGATIVE'\n",
        "      elif predictions_labels==[1]:\n",
        "        predictions_labels='POSITIVE'\n",
        " \n",
        "      # Return all true labels and prediciton for future evaluations.\n",
        "      return predictions_labels,probs\n",
        "    \n",
        "    def imports_and_installs(self):\n",
        "        \n",
        "        #Install all the required packages\n",
        "        !pip install transformers\n",
        "        !pip install numpy as np\n",
        "        !pip install pandas as pd\n",
        "          \n",
        "\n",
        "\n",
        "       \n",
        "\n",
        "    def __init__(self,Input_tweet1):\n",
        "        \n",
        "        \n",
        "        \n",
        "        #Input tweet\n",
        "        #self.imports_and_installs()\n",
        "        \n",
        "\n",
        "        #Import all the Libraries\n",
        "        import os\n",
        "        import csv\n",
        "        import io\n",
        "        import math\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import torch\n",
        "        from torch.utils.data import Dataset, DataLoader\n",
        "        from pathlib import Path\n",
        "        from tqdm.notebook import tqdm\n",
        "        from transformers import (set_seed,TrainingArguments,Trainer,GPT2Config,GPT2Tokenizer,AdamW,get_linear_schedule_with_warmup,GPT2ForSequenceClassification)\n",
        "    \n",
        "        global Input_tweet\n",
        "        Input_tweet=Input_tweet1\n",
        "        # Setting model parameters\n",
        "        epochs = 4\n",
        "        batch_size = 8\n",
        "        max_length = 60\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model_name_or_path = 'gpt2'\n",
        "        labels_ids = {'neg': 0, 'pos': 1}\n",
        "        n_labels = len(labels_ids)\n",
        "\n",
        "        # download model, config, tokenizer etc\n",
        "\n",
        "        # Get model configuration.\n",
        "        model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
        "\n",
        "        # Get model's tokenizer.\n",
        "        \n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
        "        # default to left padding\n",
        "        tokenizer.padding_side = \"left\"\n",
        "        # Define PAD Token = EOS Token = 50256\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "        # Get the actual model.\n",
        "        model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
        "\n",
        "        # resize model embedding to match new tokenizer\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "        # fix model padding token id\n",
        "        model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "        # Load model to defined device.\n",
        "        model.to(device)\n",
        "\n",
        "\n",
        "        #Load the saved model\n",
        "        #Sentiment Hugging Face\n",
        "        import os\n",
        "        Modelpath=os.getcwd()\n",
        "        \n",
        "        the_model = torch.load(Modelpath+ '\\\\SentimentModelParameters.pt', map_location=torch.device('cpu'))\n",
        "        #the_model = torch.load('/content/drive/MyDrive/BT_data/SOAmodel.pt', map_location=torch.device('cpu'))\n",
        "        \n",
        "        ###############################################################################\n",
        "    \n",
        "        #Dataloader for patagonia dataset \n",
        "        gpt2_classificaiton_collator = self.Gpt2Collator(use_tokenizer=tokenizer, labels_encoder=labels_ids, max_sequence_len=max_length)\n",
        "        train_dataset = self.Inputdatareader()\n",
        "        gold_dataset =  self.DataConverter(train_dataset,train=None, use_tokenizer=tokenizer)\n",
        "        gold_dataloader= DataLoader(gold_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
        "        #global Stance_,Probability_Score_\n",
        "        self.Stance_,self.Probability_Scores_ = self.test( gold_dataloader,the_model, device)\n",
        "        #print('This tweet expresses a ' + str(Stance_)+' Sentiment towards this topic and the Probability_Score= '+ str(Probability_Score_))\n",
        "         \n",
        "        \n",
        "        ###############################################################################\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}